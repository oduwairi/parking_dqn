# Autonomous Parking with Deep Q-Learning (DQN)

**Research Implementation**: Methodology on Autonomous Parking with Obstacle and Situational Adaptability: A Deep Q Learning Approach

## Project Overview

This project implements a Deep Q-Learning approach for autonomous parking in a 2D simulated environment with static and dynamic obstacles. The agent learns to navigate and park successfully using reinforcement learning techniques.

## Phase-by-Phase Implementation Roadmap

### Phase 1: Environment Setup & Basic Simulation ğŸ—ï¸
**Duration**: 3-5 days  
**Status**: ğŸš§ IN PROGRESS

**Objectives**:
- Set up the OpenAI Gym-compatible simulation environment
- Implement 2D parking lot layout with basic rendering
- Create the car agent with kinematic motion model
- Implement basic state space representation

**Key Components**:
- [x] Project structure and dependencies (`requirements.txt`)
- [x] Basic 2D parking environment class
- [x] Car agent with position, orientation, velocity
- [x] Kinematic motion equations implementation:
  - `x_{t+1} = x_t + v_t * cos(Î¸_t) * Î”t`
  - `y_{t+1} = y_t + v_t * sin(Î¸_t) * Î”t` 
  - `Î¸_{t+1} = Î¸_t + (v_t/L) * tan(Î´_t) * Î”t`
- [x] Basic visualization and rendering
- [x] Simple state vector: `[x_t, y_t, Î¸_t, v_t, d_1...d_8]`

**Files Created**:
- [x] `src/environment/parking_env.py` - Main environment class
- [x] `src/environment/car_agent.py` - Car physics and kinematics
- [x] `src/environment/renderer.py` - Visualization components
- [x] `requirements.txt` - Project dependencies
- [x] `test_phase1.py` - Phase 1 testing script

**Next Steps**:
1. **Install Dependencies**: `pip install -r requirements.txt`
2. **Test Phase 1**: `python test_phase1.py`
3. **Verify**: Ensure all tests pass and pygame window displays correctly

---

### Phase 2: Action Space & Basic Interaction ğŸ®
**Duration**: 2-3 days  
**Status**: âœ… COMPLETE

**Objectives**:
- Implement discrete action space (7 actions as per paper)
- Add distance sensors for obstacle detection
- Create basic reward function structure
- Test agent-environment interaction loop

**Key Components**:
- [x] Action space implementation with 7 discrete actions:
  - `a_0`: Hold/Brake (Î”v = -0.6 m/s, Î”Î´ = 0Â°)
  - `a_1`: Throttle forward (Î”v = +0.6 m/s, Î”Î´ = 0Â°)
  - `a_2`: Reverse back (Î”v = -0.6 m/s, Î”Î´ = 0Â°)
  - `a_3`: Left forward (Î”v = +0.6 m/s, Î”Î´ = +8Â°)
  - `a_4`: Right forward (Î”v = +0.6 m/s, Î”Î´ = -8Â°)
  - `a_5`: Left reverse (Î”v = -0.6 m/s, Î”Î´ = +8Â°)
  - `a_6`: Right reverse (Î”v = -0.6 m/s, Î”Î´ = -8Â°)
- [x] 8-directional distance sensors (ultrasonic/LiDAR simulation)
- [x] Basic reward function framework
- [x] Episode termination conditions

**Files Created**:
- [x] `src/environment/action_space.py` - Action definitions and logic
- [x] `src/environment/sensors.py` - Distance sensor implementations
- [x] `src/environment/rewards.py` - Reward function calculations
- [x] `test_phase2.py` - Phase 2 testing script

**Test Results**:
1. **Action Space Tests**: âœ… PASSED - All 7 actions implemented per paper specifications
2. **Sensor System Tests**: âœ… PASSED - 8-directional sensors with boundary detection
3. **Reward Function Tests**: âœ… PASSED - Collision (-100), Success (+100), Progress (Â±1/-0.5), Time (-0.1)
4. **Environment Integration**: âœ… PASSED - Full integration with modular components
5. **Interactive Demo**: âœ… PASSED - Real-time visualization with sensor rays

**Next Steps**:
1. **Completed**: All Phase 2 objectives achieved
2. **Test Phase 2**: `python test_phase2.py` âœ… ALL TESTS PASS
3. **Begin Phase 3**: Static Obstacles & Reward Engineering

---

### Phase 3: Static Obstacles & Reward Engineering ğŸš§
**Duration**: 3-4 days  
**Status**: ğŸ“‹ Planned

**Objectives**:
- Add static obstacles (barriers, stationary vehicles)
- Implement comprehensive reward function
- Create parking spot detection and validation
- Test collision detection system

**Key Components**:
- [ ] Static obstacle placement and collision detection
- [ ] Comprehensive reward function:
  - Collision penalty: -100 (episode termination)
  - Success reward: +100 (episode termination) 
  - Progress reward: +1 (closer to target), -0.5 (further)
  - Time penalty: -0.1 (per timestep)
- [ ] Parking spot validation (position tolerance Îµ_p = 0.5m, orientation tolerance Îµ_Î¸ = 10Â°)
- [ ] Episode management and reset functionality

**Files to Create**:
- `src/environment/obstacles.py` - Static obstacle management
- `src/environment/parking_spots.py` - Parking validation logic
- `src/environment/collision_detection.py` - Collision detection system

---

### Phase 4: DQN Network Architecture ğŸ§ 
**Duration**: 4-5 days  
**Status**: âœ… COMPLETE

**Objectives**:
- Implement main and target DQN networks
- Create neural network architecture (3 hidden layers, 256 neurons each)
- Implement experience replay buffer
- Set up training infrastructure

**Key Components**:
- [x] DQN network architecture:
  - Input: State vector [x, y, Î¸, v, d_1...d_8] (12 dimensions)
  - Hidden layers: 3 layers Ã— 256 neurons + ReLU activation
  - Output: Q-values for 7 actions
- [x] Main network and target network (Î¸ and Î¸â»)
- [x] Experience replay buffer (capacity ~10âµ transitions)
- [x] Huber loss function implementation
- [x] Epsilon-greedy policy with exponential decay
- [x] Double DQN implementation (reduces overestimation bias)
- [x] Prioritized experience replay (optional)

**Files Created**:
- [x] `src/dqn/network.py` - DQN network architecture
- [x] `src/dqn/replay_buffer.py` - Experience replay implementation
- [x] `src/dqn/agent.py` - DQN agent with epsilon-greedy policy
- [x] `src/dqn/loss_functions.py` - Huber loss and training utilities
- [x] `test_phase4.py` - Phase 4 testing script

**Test Results**:
1. **DQN Network Architecture**: âœ… PASSED - 3Ã—256 neural network with 136,711 parameters
2. **Experience Replay Buffer**: âœ… PASSED - Efficient circular buffer with 1MB memory usage
3. **Loss Functions & Utilities**: âœ… PASSED - Huber loss, epsilon/LR scheduling
4. **DQN Agent Integration**: âœ… PASSED - Complete agent with save/load functionality
5. **Environment Integration**: âœ… PASSED - Full integration with parking environment
6. **Performance Benchmarks**: âœ… PASSED - 207 training steps/sec, 4,395 actions/sec

**Next Steps**:
1. **Completed**: All Phase 4 objectives achieved with excellent performance
2. **Test Phase 4**: `python test_phase4.py` âœ… ALL TESTS PASS
3. **Begin Phase 5**: Training Pipeline & Hyperparameter Setup

---

### Phase 5: Training Pipeline & Hyperparameter Setup âš™ï¸
**Duration**: 3-4 days  
**Status**: âœ… COMPLETE

**Objectives**:
- Implement complete training loop
- Set up hyperparameter configuration
- Add training monitoring and logging
- Implement target network soft updates

**Key Components**:
- [x] Training hyperparameters (as per paper):
  - Learning rate Î± = 10â»Â³
  - Discount factor Î³ = 0.9-0.95
  - Batch size B = 64
  - Target update frequency N = 1000
  - Soft update rate Ï„ = 10â»Â³
- [x] Epsilon decay: `Îµ_t = Îµ_min + (Îµ_max - Îµ_min) * exp(-Î»t)`
- [x] Training loop with gradient descent and backpropagation
- [x] Model checkpointing and saving
- [x] Training metrics logging (rewards, losses, epsilon values)

**Files to Create**:
- `src/training/trainer.py` - Main training loop
- `src/training/config.py` - Hyperparameter configuration
- `src/training/logger.py` - Training metrics and logging
- `src/training/checkpoint.py` - Model saving/loading utilities

---

### Phase 6: Dynamic Obstacles & Advanced Scenarios ğŸš¶â€â™‚ï¸ğŸš—
**Duration**: 4-5 days  
**Status**: ğŸ“‹ Planned

**Objectives**:
- Add dynamic obstacles (moving pedestrians, cars)
- Create diverse training scenarios
- Implement advanced parking configurations
- Test robustness with varying conditions

**Key Components**:
- [ ] Dynamic obstacle system (pedestrians, moving vehicles)
- [ ] Multiple parking lot configurations
- [ ] Random scenario generation for training diversity
- [ ] Advanced collision avoidance scenarios
- [ ] Varying parking spot sizes and orientations

**Files to Create**:
- `src/environment/dynamic_obstacles.py` - Moving obstacle system
- `src/environment/scenario_generator.py` - Random scenario creation
- `src/environment/advanced_layouts.py` - Complex parking configurations

---

### Phase 7: Training Execution & Optimization ğŸƒâ€â™‚ï¸
**Duration**: 5-7 days  
**Status**: âœ… COMPLETE

**Objectives**:
- Execute full training (5000 episodes)
- Monitor convergence and performance
- Optimize hyperparameters if needed
- Generate training analytics and curves

**Key Components**:
- [x] Progressive training system with 3 stages:
  - Stage 1: Simple environment (no obstacles, 1000 episodes)
  - Stage 2: Add obstacles (1500 episodes)
  - Stage 3: Full complexity with random targets (2000 episodes)
- [x] Transfer learning between stages (models build on previous stage)
- [x] GPU acceleration support with automatic fallback to CPU
- [x] Real-time visualization during training (always-on pygame window)
- [x] Comprehensive training monitoring and logging
- [x] Automatic progression criteria and success/failure detection
- [x] Training curve generation and performance analytics
- [x] Early stopping and convergence detection
- [x] Model checkpointing and best model selection

**Files Created**:
- [x] Enhanced `src/training/train_with_viz.py` - Progressive training with visualization
- [x] Extended `src/training/config.py` - Progressive stage configurations
- [x] Enhanced `src/training/trainer.py` - GPU support and comprehensive logging

**Training Configurations**:
- [x] `progressive_simple`: 1000 episodes, no obstacles, fixed target
- [x] `progressive_obstacles`: 1500 episodes, with obstacles, transfer learning
- [x] `progressive_full`: 2000 episodes, full complexity, transfer learning
- [x] GPU-optimized settings with large batch sizes and efficient memory usage

**Usage Examples**:
```bash
# Full progressive training (3 stages with transfer learning)
python src/training/train_with_viz.py --mode progressive

# Single stage training with visualization
python src/training/train_with_viz.py --mode single --stage progressive_simple

# Start from specific stage (e.g., stage 2 if stage 1 already complete)
python src/training/train_with_viz.py --mode progressive --start-stage 1
```

**Key Features Implemented**:
- ğŸ® **Always-on visualization**: Pygame window shows agent behavior every episode
- ğŸ”„ **Transfer learning**: Each stage builds on previous stage's learned weights
- ğŸ“Š **Comprehensive logging**: Real-time metrics, training curves, performance tracking
- ğŸ¯ **Automatic progression**: Stages advance based on success criteria (e.g., â‰¥20% success for Stage 1)
- ğŸš€ **GPU acceleration**: Automatic CUDA detection with CPU fallback
- ğŸ“ˆ **Training analytics**: Loss curves, reward progression, success/collision rates
- ğŸ’¾ **Smart checkpointing**: Best models saved, training resumable from any point

**Next Steps**:
1. **Completed**: Progressive training system fully operational
2. **Current**: GPU PyTorch installation for acceleration
3. **Begin Phase 8**: Evaluation & Testing Framework

---

### Phase 8: Evaluation & Testing Framework ğŸ“Š
**Duration**: 3-4 days  
**Status**: ğŸ“‹ Planned

**Objectives**:
- Implement comprehensive evaluation metrics
- Create test scenarios separate from training
- Measure success rates, collision rates, timing
- Generate performance reports

**Key Components**:
- [ ] Evaluation metrics implementation:
  - Success rate (target: â‰¥70%)
  - Average time to park
  - Collision count (target: â‰¤1%)
  - Parking accuracy (position and orientation errors)
- [ ] Test scenario generation (different from training)
- [ ] Statistical analysis and confidence intervals
- [ ] Performance comparison with baseline methods

**Files to Create**:
- `src/evaluation/metrics.py` - Evaluation metrics calculation
- `src/evaluation/test_scenarios.py` - Test case generation
- `src/evaluation/evaluator.py` - Main evaluation pipeline
- `src/evaluation/report_generator.py` - Performance reporting

---

### Phase 9: Visualization & Analysis Tools ğŸ“ˆ
**Duration**: 2-3 days  
**Status**: ğŸ“‹ Planned

**Objectives**:
- Create visualization tools for agent behavior
- Implement trajectory plotting and analysis
- Generate research-quality plots and charts
- Create demo videos of successful parking

**Key Components**:
- [ ] Agent trajectory visualization
- [ ] Parking success/failure analysis plots
- [ ] Q-value heatmaps and decision analysis
- [ ] Training progress visualization
- [ ] Demo video generation capability

**Files to Create**:
- `src/visualization/trajectory_plotter.py` - Path visualization
- `src/visualization/performance_plots.py` - Metrics visualization
- `src/visualization/demo_generator.py` - Video/GIF creation

---

### Phase 10: Documentation & Final Integration ğŸ“š
**Duration**: 2-3 days  
**Status**: ğŸ“‹ Planned

**Objectives**:
- Complete documentation and user guides
- Final code cleanup and optimization
- Integration testing and bug fixes
- Prepare research artifacts and reproducibility package

**Key Components**:
- [ ] Complete API documentation
- [ ] User guide and setup instructions
- [ ] Reproducibility package with trained models
- [ ] Final performance validation
- [ ] Code quality improvements and refactoring

---

## Project Structure

```
parking_dqn/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ setup.py                          # Package setup
â”œâ”€â”€ test_phase1.py                    # Phase 1 testing script
â”œâ”€â”€ test_phase2.py                    # Phase 2 testing script
â”œâ”€â”€ test_phase3.py                    # Phase 3 testing script
â”œâ”€â”€ test_phase4.py                    # Phase 4 testing script âœ… NEW
â”œâ”€â”€ config/                           # Configuration files
â”‚   â”œâ”€â”€ training_config.yaml
â”‚   â”œâ”€â”€ environment_config.yaml
â”‚   â””â”€â”€ evaluation_config.yaml
â”œâ”€â”€ src/                              # Source code
â”‚   â”œâ”€â”€ __init__.py                   # âœ… Created
â”‚   â”œâ”€â”€ environment/                  # Simulation environment
â”‚   â”‚   â”œâ”€â”€ __init__.py               # âœ… Created
â”‚   â”‚   â”œâ”€â”€ parking_env.py           # âœ… Created - Main environment
â”‚   â”‚   â”œâ”€â”€ car_agent.py             # âœ… Created - Car physics
â”‚   â”‚   â”œâ”€â”€ renderer.py              # âœ… Created - Visualization
â”‚   â”‚   â”œâ”€â”€ action_space.py          # âœ… Created - Action definitions
â”‚   â”‚   â”œâ”€â”€ sensors.py               # âœ… Created - Distance sensors
â”‚   â”‚   â”œâ”€â”€ obstacles.py             # âœ… Created - Static obstacles
â”‚   â”‚   â”œâ”€â”€ parking_spots.py         # âœ… Created - Parking validation
â”‚   â”‚   â”œâ”€â”€ collision_detection.py   # âœ… Created - Collision system
â”‚   â”‚   â”œâ”€â”€ rewards.py               # âœ… Created - Reward functions
â”‚   â”‚   â”œâ”€â”€ dynamic_obstacles.py     # Moving obstacles
â”‚   â”‚   â”œâ”€â”€ scenario_generator.py    # Random scenarios
â”‚   â”‚   â””â”€â”€ advanced_layouts.py      # Complex layouts
â”‚   â”œâ”€â”€ dqn/                         # Deep Q-Network
â”‚   â”‚   â”œâ”€â”€ __init__.py               # âœ… Created
â”‚   â”‚   â”œâ”€â”€ network.py               # âœ… Created - DQN architecture
â”‚   â”‚   â”œâ”€â”€ agent.py                 # âœ… Created - DQN agent
â”‚   â”‚   â”œâ”€â”€ replay_buffer.py         # âœ… Created - Experience replay
â”‚   â”‚   â””â”€â”€ loss_functions.py        # âœ… Created - Loss functions
â”‚   â”œâ”€â”€ training/                    # Training pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py               # âœ… Created
â”‚   â”‚   â”œâ”€â”€ trainer.py               # Main training loop
â”‚   â”‚   â”œâ”€â”€ train_main.py            # Training script
â”‚   â”‚   â”œâ”€â”€ config.py                # Hyperparameters
â”‚   â”‚   â”œâ”€â”€ logger.py                # Logging utilities
â”‚   â”‚   â””â”€â”€ checkpoint.py            # Model management
â”‚   â”œâ”€â”€ evaluation/                  # Testing & evaluation
â”‚   â”‚   â”œâ”€â”€ __init__.py               # âœ… Created
â”‚   â”‚   â”œâ”€â”€ evaluator.py             # Main evaluation
â”‚   â”‚   â”œâ”€â”€ metrics.py               # Performance metrics
â”‚   â”‚   â”œâ”€â”€ test_scenarios.py        # Test cases
â”‚   â”‚   â””â”€â”€ report_generator.py      # Reporting
â”‚   â”œâ”€â”€ visualization/               # Visualization tools
â”‚   â”‚   â”œâ”€â”€ __init__.py               # âœ… Created
â”‚   â”‚   â”œâ”€â”€ trajectory_plotter.py    # Path plotting
â”‚   â”‚   â”œâ”€â”€ performance_plots.py     # Metrics plots
â”‚   â”‚   â””â”€â”€ demo_generator.py        # Video generation
â”‚   â””â”€â”€ analysis/                    # Analysis tools
â”‚       â”œâ”€â”€ __init__.py               # âœ… Created
â”‚       â”œâ”€â”€ training_monitor.py      # Training monitoring
â”‚       â””â”€â”€ plot_results.py          # Result plotting
â”œâ”€â”€ tests/                           # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_environment.py
â”‚   â”œâ”€â”€ test_dqn.py
â”‚   â”œâ”€â”€ test_training.py
â”‚   â””â”€â”€ test_evaluation.py
â”œâ”€â”€ models/                          # Saved models
â”‚   â””â”€â”€ checkpoints/                 # âœ… Created
â”œâ”€â”€ data/                            # Training data
â”‚   â”œâ”€â”€ training_logs/               # âœ… Created
â”‚   â””â”€â”€ evaluation_results/          # âœ… Created
â”œâ”€â”€ notebooks/                       # Jupyter notebooks
â”‚   â”œâ”€â”€ exploration.ipynb
â”‚   â”œâ”€â”€ analysis.ipynb
â”‚   â””â”€â”€ visualization.ipynb
â””â”€â”€ docs/                           # Documentation
    â”œâ”€â”€ api_reference.md
    â”œâ”€â”€ user_guide.md
    â””â”€â”€ research_notes.md
```

## Key Research Parameters (From Paper)

### State Space
- Position: (x, y) coordinates
- Orientation: Î¸ (angle)
- Velocity: v
- Distance sensors: dâ‚...dâ‚ˆ (8 directional sensors)
- **Vector**: `s_t = [x_t, y_t, Î¸_t, v_t, d_1...d_8]áµ€`

### Action Space (7 discrete actions)
| ID | Action | Description | Î”Î´ (steer) | Î”v (m/s) |
|----|--------|-------------|------------|----------|
| 0  | aâ‚€     | Hold (brake) | 0Â° | -0.6 |
| 1  | aâ‚     | Throttle forward | 0Â° | +0.6 |
| 2  | aâ‚‚     | Reverse back | 0Â° | -0.6 |
| 3  | aâ‚ƒ     | Left forward | +8Â° | +0.6 |
| 4  | aâ‚„     | Right forward | -8Â° | +0.6 |
| 5  | aâ‚…     | Left reverse | +8Â° | -0.6 |
| 6  | aâ‚†     | Right reverse | -8Â° | -0.6 |

### Reward Function
- **Collision Penalty**: -100 (episode termination)
- **Success Reward**: +100 (episode termination)
- **Progress Reward**: +1 (closer to target), -0.5 (further)
- **Time Penalty**: -0.1 (per timestep)
- **Tolerances**: Position Îµ_p = 0.5m, Orientation Îµ_Î¸ = 10Â°

### Hyperparameters
- **Learning Rate**: Î± = 10â»Â³
- **Discount Factor**: Î³ = 0.9-0.95
- **Batch Size**: B = 64
- **Replay Buffer**: ~10âµ transitions
- **Target Update**: Every N = 1000 steps
- **Soft Update Rate**: Ï„ = 10â»Â³
- **Training Episodes**: 5000
- **Network Architecture**: 3 hidden layers Ã— 256 neurons + ReLU

### Success Criteria
- **Success Rate**: â‰¥70%
- **Collision Rate**: â‰¤1%
- **Training Convergence**: Stable improvement over episodes

## Getting Started

### 1. Clone and Setup
```bash
git clone <repository-url>
cd parking_dqn
pip install -r requirements.txt
```

### 2. Test Phase 1 Installation
```bash
python test_phase1.py
```

This will run a comprehensive test of the Phase 1 implementation including:
- âœ… Environment setup verification
- âœ… Car physics and action testing
- âœ… Reward system validation
- âœ… Interactive visualization demo

### 3. Expected Output
If Phase 1 is working correctly, you should see:
- Console output showing all tests passing
- A pygame window displaying the parking environment
- A blue car (agent) and green parking spot (target)
- Real-time position, orientation, and sensor data

### 4. Next Steps
Once Phase 1 tests pass:
1. **Document any issues** in `docs/research_notes.md`
2. **Update this README** with Phase 1 completion status
3. **Begin Phase 2** implementation (action space & sensors)

## Phase 1 Troubleshooting

### Common Issues:
1. **Pygame not displaying**: Ensure you have a display available
2. **Import errors**: Verify all dependencies installed correctly
3. **Performance issues**: Check that numpy/torch are properly installed

### Debug Commands:
```bash
# Test basic imports
python -c "import src.environment.parking_env; print('âœ… Imports working')"

# Test without visualization
python -c "from src.environment.parking_env import ParkingEnv; env = ParkingEnv(); print('âœ… Environment created')"
```

## Research Notes

This implementation follows the methodology described in "Methodology on Autonomous Parking with Obstacle and Situational Adaptability: A Deep Q Learning Approach" by Osama ALDuwairi. The project aims to validate the proposed DQN approach for autonomous parking with measurable success criteria.

---

**Current Status**: Phase 4 implementation complete and tested. DQN network architecture with 136,711 parameters fully operational and integrated with autonomous parking environment. Next step is to implement the complete training pipeline (Phase 5). 